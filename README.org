check if it's possible to have more than one batch on one gpu

pls I hope distributed strategies are going to work, multinode works

Llama 2 produces gibberish with LoRA

implement brio based on Vasek's code

create evaluation pipeline with AlignScore

